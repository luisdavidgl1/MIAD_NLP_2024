{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ab6de9-d904-4673-8181-72794d7abb4e",
   "metadata": {},
   "source": [
    "# MNIST Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb21e40-2908-446a-b749-acd857d633b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Print 16 images from the MNIST dataset with their predictions\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(16):\n",
    "    axes[i].imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a09c3-1ba5-4e46-9360-347c026eca25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Define a neural network with one layer\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer=SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot the loss function for training and validation sets\n",
    "plt.plot(history.history['loss'], label='Training Loss (One Layer)')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss (One Layer)')\n",
    "plt.title('Loss Functions')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7116e5-751d-4c93-847e-b259af3b8eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "predictions = model.predict(x_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "y_trues = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Find misclassification examples\n",
    "misclassified_indices = np.where(predicted_labels != y_trues)[0]\n",
    "\n",
    "# Print the first 16 misclassification examples\n",
    "num_examples = 16\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    image_index = misclassified_indices[i]\n",
    "    image = x_test[image_index].reshape(28, 28)\n",
    "    true_label = y_trues[image_index]\n",
    "    predicted_label = predicted_labels[image_index]\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(f'True: {true_label}\\nPredicted: {predicted_label}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543c3ad-f508-4f63-9739-a0660ee3872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multi-layer perceptron (MLP) with more parameters\n",
    "mlp_model = Sequential()\n",
    "mlp_model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "mlp_model.add(Dense(512, activation='relu'))\n",
    "mlp_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile and train the MLP model\n",
    "mlp_model.compile(optimizer=SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "mlp_history = mlp_model.fit(x_train, y_train, batch_size=128, epochs=30, validation_data=(x_test, y_test))\n",
    "\n",
    "plt.plot(mlp_history.history['loss'], label='Training Loss (MLP)')\n",
    "plt.plot(mlp_history.history['val_loss'], label='Validation Loss (MLP)')\n",
    "plt.title('Loss Functions')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7449ed9-5dee-4dd8-987c-dc8c9c659c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print 16 images from the MNIST dataset with their predictions\n",
    "predictions = model.predict(x_test[:16])\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(16):\n",
    "    axes[i].imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    axes[i].set_title(f\"Predicted: {predicted_labels[i]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a69a2-f561-436d-9a11-bbea3583ae6b",
   "metadata": {},
   "source": [
    "# Dataset de noticias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9b27d-af4d-45a8-b6c8-4aeb0aa19a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "df_educacion = pd.read_csv(\"noticias_educacion_sample.csv\")\n",
    "df_educacion['clase'] = 0\n",
    "df_politica = pd.read_csv(\"noticias_politica_sample.csv\")\n",
    "df_politica['clase'] = 1\n",
    "df_deportes = pd.read_csv(\"noticias_deportes_sample.csv\")\n",
    "df_deportes['clase'] = 2\n",
    "df_economia = pd.read_csv(\"noticias_economia_sample.csv\")\n",
    "df_economia['clase'] = 3\n",
    "df = pd.concat([df_educacion, df_politica, df_deportes, df_economia]).dropna().reset_index()\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.content, df.clase, test_size=0.2, random_state=42)\n",
    "\n",
    "# Download the NLTK Spanish stopwords if you haven't already\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=2000, stop_words=spanish_stopwords, ngram_range=(1,5), lowercase=True)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Random Forest classifier\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions with Random Forest\n",
    "rf_predictions = rf_model.predict(X_test_tfidf)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "\n",
    "# Train a neural network with more layers and neurons\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=X_train_tfidf.shape[1]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(df.clase), activation='softmax'))\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_tfidf.toarray(), y_train, batch_size=128, epochs=10, validation_data=(X_test_tfidf.toarray(), y_test))\n",
    "\n",
    "# Make predictions with the neural network\n",
    "nn_predictions = np.argmax(model.predict(X_test_tfidf.toarray()), axis=-1)\n",
    "nn_accuracy = accuracy_score(y_test, nn_predictions)\n",
    "print(f\"Neural Network Accuracy: {nn_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b97a7d-de02-4444-be3d-fe35ef3a4dbf",
   "metadata": {},
   "source": [
    "# Overfitting and regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576512e4-617d-4134-96af-c8580ac11781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import History\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=2, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the MLP model with increased epochs\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=300, verbose=0)\n",
    "\n",
    "# Extract training and validation losses from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label='Train Loss')\n",
    "plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef0290-b380-4df7-8bb4-ed4f62e9e80f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Define the MLP model with dropout regularization\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=2, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout layer with dropout rate of 0.5\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout layer with dropout rate of 0.5\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the MLP model with increased epochs\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=300, verbose=0)\n",
    "\n",
    "# Extract training and validation losses from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label='Train Loss')\n",
    "plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves (with Dropout Regularization)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6b477-5a91-4910-b0e8-c99c482548db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "# Define the MLP model with L2 regularization\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the MLP model with increased epochs\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=300, verbose=0)\n",
    "\n",
    "# Extract training and validation losses from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label='Train Loss')\n",
    "plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves (with L2 Regularization)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee3736-98e8-40be-a598-a8e90c4bd25e",
   "metadata": {},
   "source": [
    "# CIFAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df52425-dfdf-4d04-808b-87858542ef15",
   "metadata": {},
   "source": [
    "CIFAR classes: 1. Airplane, 2.Automobile, 3.Bird, 4.Cat, 5.Deer, 6.Dog, 7.Frog, 8.Horse, 9.Ship, 10.Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5581b53-df5b-41ce-ac8b-ed946a5745f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train), (_, _) = cifar10.load_data()\n",
    "\n",
    "# Define the class labels\n",
    "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Plot the example images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(14):\n",
    "    image = X_train[i]\n",
    "    label = class_labels[y_train[i][0]]\n",
    "    \n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(label)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a555b-4d9d-480f-ba51-843674e17f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "y_train_nn = to_categorical(y_train, num_classes=10)\n",
    "y_test_nn = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Reshape the input data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Create a neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(3072,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_nn, epochs=10, batch_size=32, validation_data=(X_test, y_test_nn))\n",
    "\n",
    "# Evaluate the model\n",
    "_, nn_accuracy = model.evaluate(X_test, y_test_nn)\n",
    "print(\"Neural Network Accuracy:\", nn_accuracy)\n",
    "\n",
    "# Make predictions using the Neural Network\n",
    "nn_pred = model.predict(X_test)\n",
    "nn_pred_labels = np.argmax(nn_pred, axis=1)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
    "rf.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Make predictions using Random Forest\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest classifier\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1-score for each class\n",
    "target_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "nn_report = classification_report(np.argmax(y_test_nn, axis=1), nn_pred_labels, target_names=target_names)\n",
    "rf_report = classification_report(y_test, rf_pred, target_names=target_names)\n",
    "\n",
    "print(\"Neural Network Classification Report:\")\n",
    "print(nn_report)\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(rf_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
